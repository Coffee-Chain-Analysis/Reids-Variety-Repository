---
title: "Group 4 Writeup"
author: "Reid Jumper, Harsh Nagarkar, and Micheal Davis"
date: "12/4/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
# Analysis of Public Coffee Chains

## WHO SHOULD YOU INVEST INTO?

### Company Description

### Quick service restaurant

### Starbucks Corporation (formed in 1985) - Seattle Washington  - 17 countries. selling coffee, tea and deli items.

### DUNKINâ€™ BRANDS GROUP, INC. (formed in 1980) entered in market 2002. Made of Dunkin Donuts (Donuts Breakfast) Baskins and Robins (Icecreams,cakes)

### In order to compare these companies strength and fruitfulness we choose to investigate using 3 categories
### 1. US store competition analysis
### 2. Stock growth analysis 
### 3. Sec sales and stores analysis


```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(webshot)
library(tidyverse)
library(socviz)
library(usmap)
library(cowplot)
library(maps)
library(mapproj)
library(RColorBrewer)
library(lubridate)
library(tinytex)
library(knitr)
library(htmltools)
library(leaflet)
library(zoo)
#webshot::install_phantomjs

```

# Question 1: Where are the companies, Dunkin' Donuts and Starbucks, competing?

## Note: First, it is important to look at where these restaurants are located in the US.
```{r locations, message=FALSE}
sbuxloc <- read_csv("data/sbuxlocations.csv")
dnknloc <- read_csv("data/dnknlocations.csv", col_names = FALSE)
```

```{r locations_2, message=FALSE}
dnknloc <- dnknloc %>%
  rename(Longitude = X1,
         Latitude = X2,
         Specs = X3,
         Location = X4)
sbuxloc <- sbuxloc %>%
  filter(Country %in% c("US"))

dnknloc <- dnknloc %>%
  mutate(store = "Dunkin")
sbuxloc <- sbuxloc %>%
  mutate(store = "Starbucks")

dnknloc <- dnknloc %>%
  separate(Specs, c("Specs", "State"), sep = ",")

dnknloc <- dnknloc %>%
  mutate(State = dnknloc %>%
           pull(State) %>%
           str_replace_all("\\ ", ""))

dnknLabel <- sprintf("<b>Dunkin'</b><br>%s", dnknloc$State) %>%
  lapply(htmltools::HTML)
sbuxLabel <- sprintf("<b>Starbucks</b><br>%s", sbuxloc$`State/Province`) %>%
  lapply(htmltools::HTML)
```


# Dunkin' Leaflet Graph
```{r leaflet_maps_dnkn, message = FALSE}
dnknloc %>%
  leaflet(width = "100%", options = leafletOptions(zoomSnap = 0.1)) %>%
  setView(lng = -100, lat = 40, zoom = 3) %>%
  addTiles() %>%
  addMarkers(~Longitude, ~Latitude, popup = dnknLabel, label = dnknLabel)
```




# Starbucks Leaflet Graph
```{r leaflet_maps_sbux, message = FALSE}
sbuxloc %>%
  leaflet(width = "100%", options = leafletOptions(zoomSnap = 0.1)) %>%
  setView(lng = -100, lat = 40, zoom = 3) %>%
  addTiles() %>%
  addMarkers(~Longitude, ~Latitude, popup = sbuxLabel, label = sbuxLabel)
```



```{r dnkn_sbux,  message = FALSE}
dnkn_sbux <- tibble(
  longitude = c(dnknloc %>%
                  pull(Longitude), sbuxloc %>%
                  pull(Longitude)),
  latitude = c(dnknloc %>%
                 pull(Latitude), sbuxloc %>%
                 pull(Latitude)),
  store = c(dnknloc %>%
              pull(store), sbuxloc %>%
              pull(store)),
  state = c(dnknloc %>%
              pull(State), sbuxloc %>%
              pull(`State/Province`))
)

scale_coord <- function(abbreviation, x, y) {
  library(usmap)
  min_max <- read_csv("data/min_max_coord.csv")
  values <- tibble(
    abbreviation = abbreviation,
    x = x,
    y = y
  )
  usMap <- us_map(regions = "states")
  scaler <- usMap %>%
    group_by(abbr) %>%
    summarize(max.x = max(x),
              max.y = max(y),
              min.x = min(x),
              min.y = min(y))
  scaler <- scaler %>%
    mutate(variation.x = max.x - min.x,
           variation.y = max.y - min.y)
  min_max <- min_max %>%
    mutate(variation.long = max_long - min_long,
           variation.lat = max_lat - min_lat) %>%
    arrange(abbr)
  longitude <- c()
  latitude <- c()
  indexes <- c()
  count <- 1
  for(state in values$abbreviation) {
    for(abbrs in scaler$abbr) {
      if(state == abbrs) {
        indexes <- c(indexes, count)
      }
      count <- count + 1
    }
    count <- 1
  }
  count <- 1
  for(index in indexes) {
    longitude <- c(longitude, ((values$x[count]-min_max$min_long[index])/min_max$variation.long[index]) * scaler$variation.x[index] + scaler$min.x[index])
    latitude <- c(latitude, ((values$y[count]-min_max$min_lat[index])/min_max$variation.lat[index]) * scaler$variation.y[index] + scaler$min.y[index])
    count <- count + 1
  }
  return_tibble <- tibble(
    state = values$abbreviation,
    long = longitude,
    lat = latitude
  )
  return(return_tibble)
}

modified_values <- scale_coord(dnkn_sbux$state, dnkn_sbux$longitude, dnkn_sbux$latitude) %>%
  mutate(Store = dnkn_sbux$store)
```





# Subquestion: Which states have the highest number of stores for Dunkin Donuts and Starbucks?

# Locations Graphs
```{r graph_locations, message = FALSE}
ggplot() +
  geom_polygon(data = us_map(regions = "states"), mapping = aes(x = x, y = y, group = group), color = "gray", fill = "cornsilk") +
  geom_point(data = modified_values, mapping = aes(x = long, y = lat, color = Store), alpha = 0.1) +
  coord_equal() +
  theme_map() +
  scale_color_manual(values = c("blue", "orange")) +
  labs(title = "Restuarant Locations in the US",
       subtitle = "Dunkin' vs Starbucks") +
  guides(color = guide_legend(title = "Brand")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```



```{r store_per_state_starbucks, message = FALSE} 
#gather data from csv files
starbuck_local<- read_csv("data/directory.csv")
starbuck_zip<-read_csv("data/ZIP-COUNTY-FIPS-2018.csv")

#getting states dataset
us_states<-map_data("state")

#chaging dataset zip to numeric
starbuck_zip<-starbuck_zip %>% 
  mutate(ZIP=as.numeric(ZIP)) %>% 
  distinct(CITY, .keep_all = TRUE)

#filtering by country and renaming
starbuck_local <- starbuck_local %>% 
  filter(Country == "US") %>% 
  rename(
    CITY = City,
    STATE = `State/Province`
  )

#joining datasets by city
data<-left_join(starbuck_local,starbuck_zip,by=c("CITY","STATE"))

#grouping by state and removing na values
data<- data %>% 
  group_by(STATE) %>% 
  count() %>%
  na.omit() 

#renaming states to match join function
us_states<-us_map("states") %>% 
  rename(STATE=abbr)

#left joining and renaming variables
data<-left_join(us_states,data,by='STATE') %>% 
  rename(count = n)



temp_data <- data %>% 
  select(STATE, count) %>% 
  group_by(STATE) %>% 
  summarise(count = mean(count))

coord <- us_states %>% 
  select(x, y, STATE) %>% 
  group_by(STATE) %>% 
  summarise(
    x_avg = mean(x),
    y_avg = mean(y))
```


# Starbuck's Location Analysis with Top 5
```{r store_per_state_starbucks_5, message = FALSE}
new_data <- merge(temp_data, coord, by = "STATE" )

final_data <- merge(new_data, us_states, by = "STATE" )

#plot data for locations
centroid <- aggregate(data=final_data, cbind(x_avg, y_avg) ~ count + group, FUN=mean) 
data %>% 
  ggplot() +
  geom_polygon(mapping = aes(x=x,y=y,group=group,fill=count), color = "lightgray", size = 0.5) +
  coord_equal()+
  theme_map()+
  theme(panel.background = element_blank()) + ggtitle("US Starbuck's Location", subtitle = "Average # of Stores") + labs(fill = "Count")+
  theme(legend.position="bottom", plot.title = element_text(hjust = 0.5),plot.subtitle =  element_text(hjust = 0.5)) +geom_text(data = centroid, mapping = aes(x_avg,y_avg, group = group, label = count), color = "black", inherit.aes = FALSE)+ scale_fill_gradient(low = "orange", high = "purple", guide = guide_legend())

data %>%
  arrange(desc(count)) %>%
  distinct(STATE, count) %>% 
  head(5)

```

```{r store_per_state_dunkin, message = FALSE}
df<- read_csv("data/DD-US.csv",col_names = FALSE)
zip<-read_csv("data/ZIP-COUNTY-FIPS-2018.csv")
county_map <- county_map
us_states<-map_data("state")

zip<-zip %>% 
  mutate(ZIP=as.numeric(ZIP))
  


df <- df %>%
  mutate(ZIP = sapply(strsplit(df$X4, split='|', fixed=TRUE), function(x) (x[2]))) 

df <- df %>%
  mutate(DATA = sapply(strsplit(df$ZIP, split=c(','), fixed=TRUE), function(x) (x[2]))) 

df <- df %>%
  mutate(ZIP = sapply(strsplit(df$DATA, split=c(' '), fixed=TRUE), function(x) (x[2]))) 
```


```{r store_per_state_dunkin_2, message = FALSE}
df<-df %>% 
  mutate(ZIP=as.numeric(ZIP))

data<-left_join(df,zip, by=c("ZIP")) %>% 
  group_by(STATE) %>%
  count() %>%
  na.omit()

us_states<-us_map("states") %>% 
  rename(STATE=abbr)

data<-left_join(us_states,data,by='STATE')


temp_data <- data %>% 
  select(STATE, n) %>% 
  group_by(STATE) %>% 
  summarise(n = mean(n))

coord <- us_states %>% 
  select(x, y, STATE) %>% 
  group_by(STATE) %>% 
  summarise(
    x_avg = mean(x),
    y_avg = mean(y))

new_data <- merge(temp_data, coord, by = "STATE" )

final_data <- merge(new_data, us_states, by = "STATE" )
```

# Dunkin's Location Analysis with Top 5
```{r store_per_state_starbucks_6, message = FALSE}
#plot data for locations
centroid <- aggregate(data=final_data, cbind(x_avg, y_avg) ~ n + group, FUN=mean) 

data%>% 
  ggplot() +
  geom_polygon(mapping = aes(x=x,y=y,group=group,fill=n),color = "lightgray", size = 0.5) +
  coord_equal()+
  theme_map()+
  theme(panel.background = element_blank()) + ggtitle("US Dunkin' Donut's Location", subtitle = "Average # of Stores") + labs(fill = "Average") + theme(legend.position="bottom", plot.title = element_text(hjust = 0.5),plot.subtitle =  element_text(hjust = 0.5))+
  scale_fill_gradient(low = "orange", high = "purple", guide = guide_legend()) + geom_text(data = centroid, mapping = aes(x_avg,y_avg, group = group, label = n), color = "black", inherit.aes = FALSE)

data %>%
  arrange(desc(n)) %>%
  distinct(STATE, n) %>% 
  head(5)
```

```{r store_per_state_starbucks_7,message = FALSE}
#---------------------------------------------------------------------------------------------------------------------------
# by county

df<- read_csv("data/DD-US.csv",col_names = FALSE)
zip<-read_csv("data/ZIP-COUNTY-FIPS-2018.csv")

cmap<-county_map
us_states<-map_data("state")

zip<-zip %>% 
  mutate(ZIP=as.numeric(ZIP))

df<-df %>% 
  mutate(ZIP=as.numeric(str_match(X4,"(\\d{5})")[1]))

data<-left_join(zip,df,by="ZIP") %>% 
  mutate(COUNTYNAME=str_remove(COUNTYNAME," County")) %>% 
  rename(county=COUNTYNAME) %>% 
  mutate(county=tolower(county)) %>% 
  group_by(county, STATE) %>% 
  count() %>% 
  na.omit() 

data <- data %>% 
  summarise(s= max(n)) %>% 
  arrange(desc(s)) %>% 
  na.omit() 

r <- us_map(regions = "counties")

r <- r %>% 
  rename(
    STATE = abbr
  )
r <-  r %>% 
  mutate(county = str_remove(county, " County"))

r <- r %>% 
  mutate(county = tolower(county))

r <- r %>% 
  group_by(STATE, county) 
 
data<-left_join(r,data, by = c("STATE","county"))

new_data <- data %>% 
  select(STATE, county, s) %>% 
  group_by(STATE,county) %>% 
  summarise(count= max(s)) %>% 
  arrange(desc(count)) %>% 
  na.omit() %>% 
  head(20) 
```

```{r density, message=FALSE}
sq_mileage <- read_csv("data/mileage.csv")
DNKN_loc <- read_csv("data/DD-US.csv", col_names = FALSE)
SBUX_loc <- read_csv("data/directory.csv")

SBUX_loc <- SBUX_loc %>%
  filter(Country %in% c("US")) %>%
  rename(state = `State/Province`) %>%
  count(state)
DNKN_loc <- DNKN_loc %>%
  rename(specs = X3) %>%
  separate(specs, c("specs", "state"), sep = ",")
DNKN_loc <- DNKN_loc %>%
  mutate(state = DNKN_loc %>%
           pull(state) %>%
           str_replace_all("// ", "")) %>%
  count(state)

DNKN_loc <- left_join(us_map(region = "states") %>%
                        rename(state = abbr), DNKN_loc, by = "state")
SBUX_loc <- left_join(us_map(region = "states") %>%
                        rename(state = abbr), SBUX_loc, by = "state")

sq_mileage <- sq_mileage %>%
  rename(full = state,
         state_area = mileage)

DNKN_loc <- left_join(DNKN_loc, sq_mileage, by = "full")
SBUX_loc <- left_join(SBUX_loc, sq_mileage, by = "full")

DNKN_loc <- DNKN_loc %>%
  mutate(density = n/state_area)
SBUX_loc <- SBUX_loc %>%
  mutate(density = n/state_area)
```





# Subquestion: What states have the highest densities of Dunkin' and Starbucks?
## Analysis: When Washington D.C. is included in the density plots, we noticed that the data is heavily skewed, so in order to get a proper look at the densities of these companies, Washington D.C. must be filtered out.


## Density Level Graphs

# Dunkin's Density Location
```{r DC_graphs, message = FALSE}
DNKN_loc %>%
  ggplot(mapping = aes(x = x, y = y, group = group, fill = density)) +
  geom_polygon() +
  coord_equal() +
  theme_map() +
  scale_fill_distiller(palette = "Set1") +
  labs(title = "Dunkin' Locations") +
  theme(plot.title = element_text(hjust = 0.5))
```

# Starbuck's Density Location
```{r DC_graphs_2, message = FALSE}
SBUX_loc %>%
  ggplot(mapping = aes(x = x, y = y, group = group, fill = density)) +
  geom_polygon() +
  coord_equal() +
  theme_map() +
  scale_fill_distiller(palette = "Set1") +
  labs(title = "Starbucks Locations") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r filter_DC, message = FALSE}
DNKN_loc <- DNKN_loc %>%
  filter(state %nin% c("DC"))
SBUX_loc <- SBUX_loc %>%
  filter(state %nin% c("DC"))
```

# Dunkin's Density Location Excluding DC(w/ Top 5)
```{r dens_graphs, message = FALSE}
DNKN_loc %>%
  ggplot(mapping = aes(x = x, y = y, group = group, fill = density)) +
  geom_polygon() +
  coord_equal() +
  theme_map() +
  scale_fill_distiller(palette = "Set1") +
  labs(title = "Dunkin' Locations") +
  theme(plot.title = element_text(hjust = 0.5))

DNKN_loc %>%
  distinct(state, density) %>%
  arrange(desc(density)) %>%
  top_n(5)
```


# Starbuck's Density Location Excluding DC(w/ Top 5)
```{r dens_graphs_3, message = FALSE}
SBUX_loc %>%
  ggplot(mapping = aes(x = x, y = y, group = group, fill = density)) +
  geom_polygon() +
  coord_equal() +
  theme_map() +
  scale_fill_distiller(palette = "Set1") +
  labs(title = "Starbucks Locations") +
  theme(plot.title = element_text(hjust = 0.5))

SBUX_loc %>%
  distinct(state, density) %>%
  arrange(desc(density)) %>%
  top_n(5)
```

# Subquestion: How many Dunkin Donuts are in Mississippi?

```{r mississippi_dunkin,message=FALSE}

df<- read_csv("data/DD-US.csv",col_names = FALSE)
zip<-read_csv("data/ZIP-COUNTY-FIPS-2018.csv")
county_map <- us_map("county") %>% 
  filter(full=="Mississippi")
zip<-zip %>% 
  mutate(ZIP=as.numeric(ZIP))
  
df <- df %>%
  mutate(ZIP = sapply(strsplit(df$X4, split='|', fixed=TRUE), function(x) (x[2]))) 
df <- df %>%
  mutate(DATA = sapply(strsplit(df$ZIP, split=c(','), fixed=TRUE), function(x) (x[2]))) 
df <- df %>%
  mutate(ZIP = as.numeric(sapply(strsplit(df$DATA, split=c(' '), fixed=TRUE), function(x) (x[2])))) 
data<-left_join(df,zip,by="ZIP") %>% 
  # mutate(COUNTYNAME=str_remove(COUNTYNAME," County")) %>% 
  rename(county=COUNTYNAME) %>% 
  # mutate(county=tolower(county)) %>% 
  filter(STATE=="MS") %>% 
  group_by(county) %>% 
  count()

df <- df %>%
  mutate(ZIP = as.numeric(sapply(strsplit(df$DATA, split=c(' '), fixed=TRUE), function(x) (x[2])))) 

data<-left_join(df,zip,by="ZIP") %>% 
  # mutate(COUNTYNAME=str_remove(COUNTYNAME," County")) %>% 
  rename(county=COUNTYNAME) %>% 
  # mutate(county=tolower(county)) %>% 
  filter(STATE=="MS") %>% 
  group_by(county) %>% 
  count()


left_join(county_map,data,by="county") %>% 
 ggplot() +
  geom_polygon(mapping = aes(x=x,y=y,group=group,fill=n),color = "lightgray", size = 0.5) +
  coord_equal()+
  theme_map()+
  theme(panel.background = element_blank()) + ggtitle("Mississippi Dunkin' Donut's Location", subtitle = "Average # of Stores") + labs(fill = "Average") + theme(legend.position="bottom", plot.title = element_text(hjust = 0.5),plot.subtitle =  element_text(hjust = 0.5))+
  scale_fill_gradient(low = "orange", high = "purple", guide = guide_legend())

```

## Solution: The most competition between Dunkin' and Starbucks is located primarily in the North and on the East Coast.  The least competition is in the north-west where Dunkin' does not have any stores.

# Qustion 2: Do the stocks of Starbucks and Dunkin' Donuts comply with the Warren Buffet Rule?

## Analysis: Warren claimed that over the period of time the average yearly yeild of a company is in 6-7%.
```{r stock, message = FALSE}

DNKN <- read_csv("data/modified_DNKN.csv")
SBUX <- read_csv("data/modified_SBUX.csv")

DNKN <- DNKN %>%
  mutate(stock = "DNKN")
SBUX <- SBUX %>%
  mutate(stock = "SBUX")

D_S <- tibble(
  timestamp = c(DNKN %>%
                  pull(timestamp),
                SBUX %>%
                  pull(timestamp)),
  stock = c(DNKN %>%
              pull(stock),
            SBUX %>%
              pull(stock)),
  close = c(DNKN %>%
              pull(close),
            SBUX %>%
              pull(close))
)
```






# Monthly Stock Close Analysis 
```{r monthly_stock, message = FALSE}
D_S %>%
  group_by(stock) %>%
  ggplot() +
  geom_line(mapping = aes(x = timestamp, y = close, color = stock)) +
  scale_color_manual(values = c("red", "green")) +
  geom_smooth(mapping = aes(timestamp,close,color=stock),method = "lm") +
  labs(title = "Monthly Close Values",
       x = "Year",
       y = "Close Value") +
  guides(color = guide_legend(title = "Stock Symbols")) +
  theme(plot.title = element_text(hjust = 0.5))
```




# Prediction Values for Stock Analysis

```{r percent_error, message = FALSE}
Starbucks_intercepts<-lm(SBUX$close~SBUX$timestamp)[1]$coefficient
Starbucks_monthly_average_growth_rate<-(-atan(Starbucks_intercepts[1]/Starbucks_intercepts[2]))

Dunkin_intercepts<-lm(DNKN$close~DNKN$timestamp)[1]$coefficient
Dunkin_monthly_average_growth_rate<-(-atan(Dunkin_intercepts[1]/Dunkin_intercepts[2]))


Starbucks_average_growth_rate<-(tail(SBUX,1)$close-head(SBUX,1)$close)/tail(SBUX,1)$close*100/7

Dunkin_average_growth_rate<-(tail(DNKN,1)$close-head(DNKN,1)$close)/tail(DNKN,1)$close*100/7

```

# Yearly Growth rate for Starbucks and Dunkin respectively
```{r year_values_1,message=FALSE}
Starbucks_average_growth_rate
Dunkin_average_growth_rate
```

## Solution: Dunkin exceed a bit where as starbucks is almost in range. Starbuck stock yeild is coming down due to the stock split happening in 2015.

# Subquestion: What would the monthly growth rate be for Starbucks and Dunkin be?
```{r year_values_2,message=FALSE}
Starbucks_monthly_average_growth_rate
Dunkin_monthly_average_growth_rate
```


```{r mutate_DS, message = FALSE}
yrly <- D_S %>%
  mutate(year = year(timestamp)) %>%
  group_by(year, stock) %>%
  summarize(mean_close = mean(close))
```









# Yearly Mean Stock Close Analysis 
```{r yearly_mean, message = FALSE}
yrly %>%
  group_by(stock) %>%
  ggplot() +
  geom_line(mapping = aes(x = year, y = mean_close, color = stock)) +
  scale_color_manual(values = c("red", "green")) +
  labs(title = "Average Monthly Close Value by Year",
       x = "Year",
       y = "Mean Close Value") +
  guides(color = guide_legend(title = "Stock Symbols")) +
  theme(plot.title = element_text(hjust = 0.5))

```



# Question 3: What are some patterns that show in the revenue data of either Dunkin' or Starbucks?

# Starbuck's Store Count Analysis
```{r revenue_starbucks, message = FALSE, warning = FALSE}
#Revenue of company stores
starbuck_rev<- read_csv("data/starbucks.csv")

starbuck_rev <- starbuck_rev %>% 
  select(year, Company_Operated_Stores:Total_OS)

starbuck_rev <- starbuck_rev %>% 
  rename(`Company Operated Stores`= Company_Operated_Stores, `Licensd Stores` = License_Stores, `Total-Overall` = Total)

starbuck_rev <- starbuck_rev %>% 
  select(`Company Operated Stores`, `Licensd Stores` , Other, `Total-Overall` ,year) %>% 
  gather(Type, count, `Company Operated Stores`, `Licensd Stores`, Other, `Total-Overall`) %>% 
  na.omit()



starbuck_rev <- starbuck_rev %>% 
  group_by(Type, year) %>% 
  summarise(mean = mean(count))
```

```{r revenue_starbucks_2, message = FALSE, warning = FALSE}
 starbuck_rev %>% 
   ggplot() + 
   geom_col(mapping = aes(reorder(year, mean),
                          y = mean, fill=Type)) + labs(title = "Starbuck's Store Count")+
   theme(plot.title = element_text(hjust = 0.5)) + xlab("Year") + ylab("Count")  +  facet_wrap(~Type, nrow = 3, scales = "free")
```


## Analysis: At the visual inspection from 2015 we saw the CAP segment license dropping and comapany owned increasing. On further analysis we concluded that the business model of the Starbucks store is license where company creates company owned stores and then licenses in bulk. 2015 was the only time they happened in reverse and we could track this detail down.


```{r revenue_dunkin, message = FALSE,  warning = FALSE}
dunk_rev<- read_csv("data/dunkin.csv")

dunk_rev <- dunk_rev %>% 
  select(year_of_publish, DD_US_F:BR_US_C, Franchise_income, Rental_income:Total_reveneu)

dunk_rev <- dunk_rev %>% 
  rename(
    `Dunkin US Franchise` = DD_US_F,
    `Dunkin International Franchise` = DD_IN_F,
    `Baskin Robin US Franchise` = BR_US_F,
    `Baskin Robin International Franchise` = BR_IN_F,
    `Baskin Robin US Company` = BR_US_C,
    `Dunkin US Company` = DD_US_C
  )
```


```{r revenue_dunkin_1, message = FALSE, warning = FALSE}
dunk_rev <- dunk_rev %>% 
  select(`Dunkin US Franchise`, `Dunkin International Franchise`, `Baskin Robin US Franchise`,  `Baskin Robin International Franchise`,`Baskin Robin US Company`,`Dunkin US Company`, year_of_publish) %>% 
  gather(Type, count,`Dunkin US Franchise`, `Dunkin International Franchise` , `Baskin Robin US Franchise` ,`Baskin Robin International Franchise`,`Baskin Robin US Company`,`Dunkin US Company`) %>% 
  na.omit()



dunk_rev <- dunk_rev%>% 
  group_by(Type, year_of_publish) %>% 
  summarise(mean = mean(count))
```


# Dunkin Donuts Store Count Analysis
```{r revenue_dunkin_9, message = FALSE, warning = FALSE}
dunk_rev %>% 
    ggplot() +
    geom_col(mapping = aes(reorder(year_of_publish, mean),
                           y = mean, fill=Type)) +
    theme(plot.title = element_text(hjust = 0.5)) + ylab("Store Count") + xlab("Year")+ labs(title = "Dunkin Donut's Store Count")+ facet_wrap(~Type, scales = "free")
```
 
## Analysis: At the visual inspection of the Dunkin company stores went down to zero in 2017. Upon further inspection we figured out that the Dunkin model is franchise based and it changed to 100% franchise in 2016 dropping all it's ompany operated stores.

```{r revenue_starbucks_3, message = FALSE, warning = FALSE}
starbuck_rev<- read_csv("data/starbucks.csv")

starbuck_rev <- starbuck_rev %>% 
  select(year, Company_Operated_Stores:Total_OS)

starbuck_rev <- starbuck_rev %>% 
rename(`Americas` = Americas_Licensed,
       `Europe/Middle-East Asia` = EMEA_Licensed,
       `China/Asia Pacific` = CP_Licensed,
       `Total-Licensed` = Total_LS)

starbuck_rev <- starbuck_rev %>% 
  select(`Americas`, `Europe/Middle-East Asia`, `China/Asia Pacific`, Total, year) %>% 
  gather(Type, count, Americas, `Europe/Middle-East Asia` ,`China/Asia Pacific` , Total) %>% 
  na.omit()





starbuck_rev <- starbuck_rev %>% 
  group_by(Type, year) %>% 
  summarise(mean = mean(count))
```


# Starbucks Revenue Analysis (Licensed)
```{r revenue_starbucks_12, message = FALSE, warning = FALSE}
starbuck_rev %>%
group_by(Type) %>%
  ggplot(mapping = aes(x = year, y = mean)) +
  geom_line(mapping = aes(color = Type)) + ylab("Revenue") + xlab("Year")+ facet_wrap(~Type, nrow = 3,scales = "free") + labs(title = "Starbuck's Revenue(LS)")+ theme(plot.title = element_text(hjust = 0.5))


```


```{r revenue_starbucks_5, message = FALSE, warning = FALSE}

#revenue by country(com)

starbuck_rev<- read_csv("data/starbucks.csv")

starbuck_rev <- starbuck_rev %>% 
  select(year, Company_Operated_Stores:Total_OS)

starbuck_rev <- starbuck_rev %>% 
rename(`Americas` = Americas_Com,
       `Europe/Middle-East Asia` = EMEA_Com,
       `China/Asia Pacific` = CP_Com,
       `Total-Operated Stores` = Total_OS)

starbuck_rev <- starbuck_rev %>% 
  select(Americas, `Europe/Middle-East Asia`, `China/Asia Pacific`, Total, year) %>% 
  gather(Type, count, Americas, `Europe/Middle-East Asia` ,`China/Asia Pacific` , Total) %>% 
  na.omit()



starbuck_rev <- starbuck_rev %>% 
  group_by(Type, year) %>% 
  summarise(mean = mean(count))

# starbuck_rev %>% 
#   ggplot() + 
#   geom_col(mapping = aes(reorder(year, mean),
#                          y = mean, fill=Type)) + labs(title = "Starbuck's Reveue/Country(OS)")+
#   theme(plot.title = element_text(hjust = 0.5)) + xlab("Year") + ylab("Revenue") + scale_fill_brewer(palette = "Set2")  +  facet_wrap(~Type, nrow = 3, scales = "free") + coord_flip()

```

# Starbucks Revenue Analysis (Operated Stores)

## Analysis: We found these patterns by analzing visually. By inspection, these resulted from analyzing all 10-K files data.
```{r revenue_starbucks_6, message = FALSE, warning = FALSE}
starbuck_rev %>%
group_by(Type) %>%
  ggplot(mapping = aes(x = year, y = mean)) +
  geom_line(mapping = aes(color = Type)) + ylab("Revenue") + xlab("Year")+ facet_wrap(~Type, nrow = 3,scales = "free") + labs(title = "Starbuck's Revenue(OS)")+ theme(plot.title = element_text(hjust = 0.5))


```


## - 2014 - Opens first Starbucks ReserveÂ® Roastery and Tasting Room in Seattle.

## - Launches Starbucks Mobile Order & Pay.

## - 2015 - Evolution Fresh Stores and Teavana tea massive close-downs

## - 2016- Siren Retail Group,Teavana tea dump,tazo tea,evolution fresh(germany switzerland bad euro rate)

## - 2017- Purchased all of CAP segments. Opened many stores in Americas,emas, Food, consumer packaged goods production like coffee, Keurig cups was hurting so contract to nestle. Improved Starbucks siren(Reserve store)

```{r revenue_dunkin_2, message = FALSE, warning = FALSE}
dunk_rev<- read_csv("data/dunkin.csv")

dunk_rev <- dunk_rev %>% 
  select(year_of_publish, DD_US_F:BR_IN_F, Franchise_income, Rental_income:Total_reveneu)

dunk_rev <- dunk_rev %>% 
  rename(
    `Franchise Income` = Franchise_income,
    `Rental Income` = Rental_income,
    `Sales of Company Restaruant` = `Sales_of_company restaurant`,
    `Sales of IceCream` = Sales_of_Icecream,
    `Total Revenue` = Total_reveneu,
    `Other Revenue` = Other_reveneue
 
  )
dunk_rev <- dunk_rev %>% 
  select( `Franchise Income` , `Rental Income`, `Sales of Company Restaruant`, `Sales of IceCream`,`Total Revenue`, `Other Revenue`, year_of_publish) %>% 
  gather(Type, count, `Franchise Income`, `Rental Income`, `Sales of Company Restaruant`, `Sales of IceCream`,`Total Revenue`,`Other Revenue`) %>% 
  na.omit()


dunk_rev <- dunk_rev%>% 
  group_by(Type, year_of_publish) %>% 
  summarise(mean = mean(count))
```


# Dunkin Donuts Revenue Analysis 
```{r revenue_dunkin_3, message = FALSE, warning = FALSE}
dunk_rev %>%
  group_by(Type) %>% 
  ggplot(mapping = aes(x = year_of_publish, y = mean)) +
  geom_line(mapping = aes(color = Type)) + ylab("Revenue") + xlab("Year")+ facet_wrap(~Type, scales = "free")+ theme(plot.title = element_text(hjust = 0.5))+ labs(title = "Dunkin Donut's Revenue")

```



## - 2014- Ice Cream cakes were launched and sold online with the order, and across all supermarkets. High ads were carried out. This leads to high sales for k cups and coffee also.

## - 2016- In this year they change to 100% 

## - 2018-Other related revenue increased by the addition of the gift card program

## Analysis: Overall, the main increase was due to  recognizing advertising revenue(5% of each store sales) separately from this balance sheet. The growth actually declined Other the real growth is 828027 

## Solution: Based on our analysis,   Starbucks shows higher revenue and showcases stronger adaptive policies for sustained growth.
